# !!Remember to set your working directory!!

# k-means Clustering ------------------------------------------------------

# Load packages required for this part

library(spotifyr)
library(ggplot2)

# Load the Spotify data you saved in Lab 3-1

sp_data <- readRDS("sp_data.rds")


# Create a scatter plot of popularity vs followers count

ggplot(sp_data, aes(x = popularity, y = followers.total)) + 
  geom_point(stat = "identity") + 
  xlab("Popularity") + 
  ylab("Followers Count") + 
  ggtitle("Popularity vs Followers")


# Recreate the same plot using logarithmic scale for followers

ggplot(sp_data, aes(x = (popularity), y = log(followers.total))) + 
  geom_point(stat = "identity") + 
  xlab("Popularity") + 
  ylab("Log Followers Count") + 
  ggtitle("Popularity vs Log Followers")


# Create data frame with the log data

artist_log <- data.frame(sp_data$name, sp_data$popularity, (sp_data$followers.total))
View(artist_log)


# Run the k-means clustering algorithm

artist_clusters <- kmeans(artist_log[ , 2:3],
                        centers = 7,
                        iter.max = 10,
                        nstart = 10)


# Add the cluster number to each row

artist_log$cluster <- factor(artist_clusters$cluster)
View(artist_log)


# Plot the clusters

ggplot(artist_log, aes(x = artist_log[ , 2], 
                        y = artist_log[ , 3],
                        colour = cluster)) +
  geom_point(stat = "identity") + 
  xlab("Popularity") + 
  ylab("Log Followers Count") + 
  ggtitle("Popularity vs Log Followers")


# View the cluster centres

artist_clusters$centers



# Topic Modelling ---------------------------------------------------------

# Load packages required for this part

library(tidyr)
library(tidytext)
library(textclean)
library(tm)
library(topicmodels)
library(reshape2)
library(dplyr)
library(ggplot2)


# Load a dataset you want to work with (e.g., "rd_data" or "yt_data")

rd_data <- readRDS("rd_data.rds")

# Remove rows that have 'NA'

rd_data <- rd_data[complete.cases(rd_data), ]


# Clean the text

clean_text <- rd_data$comment |> # change 'comment' to 'Comment' for YouTube
  replace_url() |> 
  replace_html() |>
  replace_non_ascii() |>
  replace_word_elongation() |>
  replace_internet_slang() |>
  replace_contraction() |>
  removeNumbers() |> 
  removePunctuation()

# Convert clean_text vector into a document corpus (collection of documents)

text_corpus <- VCorpus(VectorSource(clean_text))

text_corpus[[1]]$content
text_corpus[[5]]$content


# Perform further pre-processing 

text_corpus <- text_corpus |>
  tm_map(content_transformer(tolower)) |> 
  tm_map(removeWords, stopwords(kind = "SMART")) |> 
  # tm_map(stemDocument) |> # optional
  tm_map(stripWhitespace)

text_corpus[[1]]$content
text_corpus[[5]]$content


# Transform corpus into a Document Term Matrix and remove 0 entries

doc_term_matrix <- DocumentTermMatrix(text_corpus)
non_zero_entries = unique(doc_term_matrix$i)
dtm = doc_term_matrix[non_zero_entries,]


# Optional: Remove objects and run garbage collection for faster processing

save(dtm, file = "doc_term_matrix.RData")
rm(list = ls(all.names = TRUE))
gc() 
load("doc_term_matrix.RData")


# Create LDA model with k topics

lda_model <- LDA(dtm, k = 9)


# Generate topic probabilities for each word
# 'beta' shows the probability that this word was generated by that topic

found_topics <- tidy(lda_model, matrix = "beta")
View(found_topics)


# Visualise the top 10 terms per topic

top_terms <- found_topics |>
  group_by(topic) |>
  slice_max(beta, n = 10) |> 
  ungroup() |>
  arrange(topic, -beta)

top_terms |>
  mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

# Remember to save your data
save.image(file = "5-1_Lab_Data.RData")
