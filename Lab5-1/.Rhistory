setwd('~/Griffith/Big Data Analytics/Assignment/Test01_albums')
load("~/Griffith/Big Data Analytics/Assignment/Test01_albums/preprocess_text_taylor.RData")
reddit_actor_taylor <- readRDS("~/Griffith/Big Data Analytics/Assignment/Test01_albums/reddit_actor_taylor.rds")
network_graph <- readRDS("reddit_actor_taylor.rds")
length(V(network_graph))
library(vosonSML)
library(dplyr)
library(tidyr)
library(tidytext)
library(textclean)
library(tm)
library(ggplot2)
library(igraph)
# Load Packages for Spotify
library(spotifyr)
library(ggplot2)
library(ggridges)
length(V(network_graph))
V(network_graph)$name[1:20]
comps <- components(network_graph, mode = c("weak"))
comps$no
comps$csize
head(comps$membership, n = 30)
largest_comp <- which.max(comps$csize)
comp_subgraph <- network_graph |>
induced_subgraph(vids = which(comps$membership == largest_comp))
sort(degree(comp_subgraph, mode = "in"), decreasing = TRUE)[1:10]
sort(degree(comp_subgraph, mode = "out"), decreasing = TRUE)[1:10]
sort(degree(comp_subgraph, mode = "total"), decreasing = TRUE)[1:10]
sort(closeness(comp_subgraph, mode = "in"), decreasing = TRUE)[1:10]
sort(closeness(comp_subgraph, mode = "out"), decreasing = TRUE)[1:10]
sort(closeness(comp_subgraph, mode = "total"), decreasing = TRUE)[1:10]
sort(betweenness(comp_subgraph, directed = FALSE), decreasing = TRUE)[1:10]
undir_network_graph <- as.undirected(network_graph, mode = "collapse")
louvain_comm <- cluster_louvain(undir_network_graph)
sizes(louvain_comm)
plot(louvain_comm,
undir_network_graph,
vertex.label = V(undir_network_graph)$screen_name,
vertex.size = 4,
vertex.label.cex = 0.7)
eb_comm <- cluster_edge_betweenness(undir_network_graph)
sizes(eb_comm)
sizes(louvain_comm)
library(VOSONDash)
run(VOSONDASH)
run(VOSONDash)
runVOSONDash()
write_graph(rd_actor_graph, file = "reddit_actor_taylor.graphml", format = "graphml")
runVOSONDash()
undir_network_graph <- as.undirected(network_graph, mode = "collapse")
eb_comm <- cluster_edge_betweenness(undir_network_graph)
sizes(eb_comm)
sizes(louvain_comm)
plot(eb_comm,
undir_network_graph,
vertex.label = V(undir_network_graph)$screen_name,
vertex.size = 4,
vertex.label.cex = 0.7)
is_hierarchical(eb_comm)
as.dendrogram(eb_comm)
plot_dendrogram(eb_comm)
plot_dendrogram(eb_comm, mode = "dendogram", xlim=c(1,20))
plot_dendrogram(eb_comm, mode = "dendrogram", xlim=c(1,20))
plot_dendrogram(eb_comm, mode = "dendrogram", xlim=c(450,500))
plot_dendrogram(eb_comm, mode = "dendrogram", xlim=c(400,450))
# Saving Data for Community Analysis
save.image(file = "social_network_analysis_taylor.RData")
library(vosonSML)
library(igraph)
network_graph <- readRDS("reddit_actor_taylor.rds")
rank_yt_actor <- sort(page_rank(network_graph)$vector, decreasing = TRUE)
rank_yt_actor[1:10]
rank_yt_actor <- sort(page_rank(network_graph)$vector, decreasing = TRUE)
rank_yt_actor[1:10]
rank_yt_actor <- sort(page_rank(network_graph)$vector, decreasing = TRUE)
rank_yt_actor[1:10]
setwd('~/Griffith/Big Data Analytics/Lab5-1')
library(spotifyr)
library(ggplot2)
# !!Remember to set your working directory!!
library(spotifyr)
library(ggplot2)
library(ggridges)
library(igraph)
library(tidyr)
# Set up authentication variables
app_id <- "7e9f2297a08e4b66ba790a39a1330998"
app_secret <- "8acb49b84d0d4714865b2c0b8f85405e"
token <- "1"
# Authentication for spotifyr package:
Sys.setenv(SPOTIFY_CLIENT_ID = app_id)
Sys.setenv(SPOTIFY_CLIENT_SECRET = app_secret)
access_token <- get_spotify_access_token()
# Find artist ID on Spotify
find_my_artist <- search_spotify("Bruno Mars", type = "artist")
View(find_my_artist)
# Spotify Related Artists Graph -----------------------------------------
# Retrieve information about Bruno Mars' related artists
related_bm <- get_related_artists("0du5cEVh5yTK9QJze8zA0C")
View(related_bm)
# Create a network of artists related to Bruno Mars' related artists
edges <- c()
for (artist in related_bm$id){
related <- get_related_artists(artist)
artist_name <- get_artist(artist)$name
for (relatedartist in related$name){
edges <- append(edges, artist_name)
edges <- append(edges, relatedartist)
}
}
edges[1:10]
# Convert network to graph and save as external file
related_artists_graph <- graph(edges)
saveRDS(related_artists_graph, file = "SpotifyActor.rds")
write_graph(related_artists_graph, file = "SpotifyActor.graphml", format = "graphml")
# Spotify Related Artists Data Frame --------------------------------------
# Create a new variable to store all data about the related artists
related_all <- related_bm
for (artist in related_bm$id){
related <- get_related_artists(artist)
related_all <- rbind(related_all, related)
}
View(related_all)
# Remove duplicate rows
sp_data <- unique(related_all)
View(sp_data)
# Perform some processing on the data frame and then export as a CSV
sp_data$num_genres <- lengths(sp_data$genres) # add column: number of genres
sp_data <- sp_data[ , -4] |>      # remove list-column "images"
unnest_wider(genres, names_sep = "_")   # split list-column "genres"
saveRDS(sp_data, file = "sp_data.rds")
write.csv(sp_data, file = "sp_data.csv")
# Remember to save your data
save.image(file = "Spotify_Graphs.RData")
sp_data <- readRDS("sp_data.rds")
ggplot(sp_data, aes(x = popularity, y = followers.total)) +
geom_point(stat = "identity") +
xlab("Popularity") +
ylab("Followers Count") +
ggtitle("Popularity vs Followers")
ggplot(sp_data, aes(x = (popularity), y = log(followers.total))) +
geom_point(stat = "identity") +
xlab("Popularity") +
ylab("Log Followers Count") +
ggtitle("Popularity vs Log Followers")
artist_log <- data.frame(sp_data$name, sp_data$popularity, log(sp_data$followers.total))
View(artist_log)
artist_clusters <- kmeans(artist_log[ , 2:3],
centers = 7,
iter.max = 10,
nstart = 10)
artist_log$cluster <- factor(artist_clusters$cluster)
View(artist_log)
ggplot(artist_log, aes(x = artist_log[ , 2],
y = artist_log[ , 3],
colour = cluster)) +
geom_point(stat = "identity") +
xlab("Popularity") +
ylab("Log Followers Count") +
ggtitle("Popularity vs Log Followers")
# Run the k-means clustering algorithm
artist_clusters <- kmeans(artist_log[ , 2:3],
centers = 3,
iter.max = 10,
nstart = 10)
# Add the cluster number to each row
artist_log$cluster <- factor(artist_clusters$cluster)
View(artist_log)
# Plot the clusters
ggplot(artist_log, aes(x = artist_log[ , 2],
y = artist_log[ , 3],
colour = cluster)) +
geom_point(stat = "identity") +
xlab("Popularity") +
ylab("Log Followers Count") +
ggtitle("Popularity vs Log Followers")
artist_log <- data.frame(sp_data$name, sp_data$popularity, (sp_data$followers.total))
View(artist_log)
artist_clusters <- kmeans(artist_log[ , 2:3],
centers = 3,
iter.max = 10,
nstart = 10)
# Add the cluster number to each row
artist_log$cluster <- factor(artist_clusters$cluster)
View(artist_log)
# Plot the clusters
ggplot(artist_log, aes(x = artist_log[ , 2],
y = artist_log[ , 3],
colour = cluster)) +
geom_point(stat = "identity") +
xlab("Popularity") +
ylab("Log Followers Count") +
ggtitle("Popularity vs Log Followers")
# Run the k-means clustering algorithm
artist_clusters <- kmeans(artist_log[ , 2:3],
centers = 7,
iter.max = 10,
nstart = 10)
# Add the cluster number to each row
artist_log$cluster <- factor(artist_clusters$cluster)
View(artist_log)
# Plot the clusters
ggplot(artist_log, aes(x = artist_log[ , 2],
y = artist_log[ , 3],
colour = cluster)) +
geom_point(stat = "identity") +
xlab("Popularity") +
ylab("Log Followers Count") +
ggtitle("Popularity vs Log Followers")
artist_clusters$centers
rd_data <- readRDS("~/Griffith/Big Data Analytics/Lab5-1/rd_data.rds")
library(tidyr)
library(tidytext)
library(textclean)
library(tm)
library(topicmodels)
library(reshape2)
library(dplyr)
library(ggplot2)
rd_data <- readRDS("rd_data.rds")
rd_data <- rd_data[complete.cases(rd_data), ]
clean_text <- rd_data$comment |> # change 'comment' to 'Comment' for YouTube
replace_url() |>
replace_html() |>
replace_non_ascii() |>
replace_word_elongation() |>
replace_internet_slang() |>
replace_contraction() |>
removeNumbers() |>
removePunctuation()
text_corpus <- VCorpus(VectorSource(clean_text))
text_corpus[[1]]$content
text_corpus[[5]]$content
text_corpus <- text_corpus |>
tm_map(content_transformer(tolower)) |>
tm_map(removeWords, stopwords(kind = "SMART")) |>
# tm_map(stemDocument) |> # optional
tm_map(stripWhitespace)
text_corpus[[1]]$content
text_corpus[[5]]$content
doc_term_matrix <- DocumentTermMatrix(text_corpus)
non_zero_entries = unique(doc_term_matrix$i)
dtm = doc_term_matrix[non_zero_entries,]
save(dtm, file = "doc_term_matrix.RData")
rm(list = ls(all.names = TRUE))
gc()
load("doc_term_matrix.RData")
lda_model <- LDA(dtm, k = 6)
found_topics <- tidy(lda_model, matrix = "beta")
library(tidyr)
library(tidytext)
library(textclean)
library(tm)
library(topicmodels)
library(reshape2)
library(dplyr)
library(ggplot2)
found_topics <- tidy(lda_model, matrix = "beta")
lda_model <- LDA(dtm, k = 6)
found_topics <- tidy(lda_model, matrix = "beta")
install.packages("tidytext")
library(tidytext)
found_topics <- tidy(lda_model, matrix = "beta")
lda_model <- LDA(dtm, k = 6)
found_topics <- tidy(lda_model, matrix = "beta")
found_topics <- tidy(lda_model, matrix = "beta")
library(tidyr)
library(tidytext)
library(textclean)
library(tm)
library(topicmodels)
library(reshape2)
library(dplyr)
library(ggplot2)
# Load a dataset you want to work with (e.g., "rd_data" or "yt_data")
rd_data <- readRDS("rd_data.rds")
# Remove rows that have 'NA'
rd_data <- rd_data[complete.cases(rd_data), ]
# Clean the text
clean_text <- rd_data$comment |> # change 'comment' to 'Comment' for YouTube
replace_url() |>
replace_html() |>
replace_non_ascii() |>
replace_word_elongation() |>
replace_internet_slang() |>
replace_contraction() |>
removeNumbers() |>
removePunctuation()
# Convert clean_text vector into a document corpus (collection of documents)
text_corpus <- VCorpus(VectorSource(clean_text))
text_corpus[[1]]$content
text_corpus[[5]]$content
# Perform further pre-processing
text_corpus <- text_corpus |>
tm_map(content_transformer(tolower)) |>
tm_map(removeWords, stopwords(kind = "SMART")) |>
# tm_map(stemDocument) |> # optional
tm_map(stripWhitespace)
text_corpus[[1]]$content
text_corpus[[5]]$content
# Transform corpus into a Document Term Matrix and remove 0 entries
doc_term_matrix <- DocumentTermMatrix(text_corpus)
non_zero_entries = unique(doc_term_matrix$i)
dtm = doc_term_matrix[non_zero_entries,]
# Optional: Remove objects and run garbage collection for faster processing
save(dtm, file = "doc_term_matrix.RData")
rm(list = ls(all.names = TRUE))
gc()
load("doc_term_matrix.RData")
lda_model <- LDA(dtm, k = 6)
found_topics <- tidy(lda_model, matrix = "beta")
View(found_topics)
top_terms <- found_topics |>
group_by(topic) |>
slice_max(beta, n = 10) |>
ungroup() |>
arrange(topic, -beta)
top_terms |>
mutate(term = reorder_within(term, beta, topic)) |>
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
# Create LDA model with k topics
lda_model <- LDA(dtm, k = 9)
# Generate topic probabilities for each word
# 'beta' shows the probability that this word was generated by that topic
found_topics <- tidy(lda_model, matrix = "beta")
View(found_topics)
# Visualise the top 10 terms per topic
top_terms <- found_topics |>
group_by(topic) |>
slice_max(beta, n = 10) |>
ungroup() |>
arrange(topic, -beta)
top_terms |>
mutate(term = reorder_within(term, beta, topic)) |>
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
# Remember to save your data
save.image(file = "5-1_Lab_Data.RData")
