View(related_bm)
save.image(file = "spotify_taylor.RData")
clean_text <- rd_data$comment |>
replace_url() |>
replace_html() |>
replace_non_ascii() |> # ` vs '
replace_word_elongation() |>
replace_internet_slang() |>
replace_contraction() |>
removeNumbers() |>
removePunctuation() # |>
text_corpus <- VCorpus(VectorSource(clean_text))
text_corpus <- text_corpus |>
tm_map(content_transformer(tolower)) |>
tm_map(removeWords, stopwords(kind = "SMART")) |>
tm_map(stemDocument) |>
tm_map(stripWhitespace)
doc_term_matrix <- DocumentTermMatrix(text_corpus)
dtm_df <- as.data.frame(as.matrix(doc_term_matrix))
View(dtm_df)
freq <- sort(colSums(dtm_df), decreasing = TRUE)
feq_10 <- head(freq, n = 10)
feq_10_df <- data.frame(
term = names(feq_10),
frequency = feq_10
)
ggplot(feq_10_df, aes(x = reorder(term, -frequency), y = frequency)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Top 10 Most Frequent Terms",
x = "Terms",
y = "Frequency") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
clean_df <- data.frame(clean_text)
rd_bigrams <- clean_df |> unnest_tokens(output = bigram,
input = clean_text,
token = "ngrams",
n = 2)
View(rd_bigrams)
rd_bigrams_table <- rd_bigrams |>
count(bigram, sort = TRUE) |>
separate(bigram, c("left", "right"))
View(rd_bigrams_table)
rd_bigrams_nostops <- rd_bigrams_table |>
anti_join(stop_words, join_by(left == word)) |>
anti_join(stop_words, join_by(right == word)) # different to above because now table
View(rd_bigrams_nostops)
rd_bigrams_nostops <- rd_bigrams_nostops[complete.cases(rd_bigrams_nostops), ]
View(rd_bigrams_nostops)
rd_bigrams_nostops <- rd_bigrams_nostops |> filter(n >= 2)
View(rd_bigrams_nostops)
rd_bigram_graph <- graph_from_data_frame(rd_bigrams_nostops, directed = FALSE)
rd_bigram_graph
vcount(rd_bigram_graph)
ecount(rd_bigram_graph)
rd_bigram_graph <- simplify(rd_bigram_graph)
vcount(rd_bigram_graph)
ecount(rd_bigram_graph)
plot(rd_bigram_graph, vertex.size = 4, edge.arrow.size = 0.5)
write_graph(rd_bigram_graph, file = "taylor_bigram.graphml", format = "graphml")
rank_rd_bigram <- sort(page_rank(rd_bigram_graph)$vector, decreasing=TRUE)
rank_rd_bigram[1:10]
save.image(file = "preprocess_text_taylor.RData")
# Remove rows of rd_data that have 'NA'
rd_data <- rd_data[complete.cases(rd_data), ]
View(rd_data)
# Create actor network
rd_actor_network <- rd_data |>
Create("actor") |>
AddText(rd_data,
verbose = TRUE)
# Create graph from the network
rd_actor_graph <- rd_actor_network |> Graph()
rd_actor_graph
V(rd_actor_graph)$name <- V(rd_actor_graph)$user
rd_actor_graph
# Save and write graph to file
saveRDS(rd_actor_graph, file = "reddit_actor_taylor.rds")
write_graph(rd_actor_graph, file = "reddit_actor_taylor.graphml", format = "graphml")
# Load your chosen network graph
network_graph <- readRDS("reddit_actor_taylor.rds")
# Inspect the graph object
length(V(network_graph))
V(network_graph)$name[1:20]
# Find all maximum components that are weakly connected
comps <- components(network_graph, mode = c("weak"))
comps$no
comps$csize
head(comps$membership, n = 30)
# Get sub-graph with most members
largest_comp <- which.max(comps$csize)
comp_subgraph <- network_graph |>
induced_subgraph(vids = which(comps$membership == largest_comp))
# Display top 10 nodes from the sub-graph ordered by degree centrality
sort(degree(comp_subgraph, mode = "in"), decreasing = TRUE)[1:10]
sort(degree(comp_subgraph, mode = "out"), decreasing = TRUE)[1:10]
sort(degree(comp_subgraph, mode = "total"), decreasing = TRUE)[1:10]
# Display top 10 nodes from the sub-graph ordered by closeness centrality
sort(closeness(comp_subgraph, mode = "in"), decreasing = TRUE)[1:10]
sort(closeness(comp_subgraph, mode = "out"), decreasing = TRUE)[1:10]
sort(closeness(comp_subgraph, mode = "total"), decreasing = TRUE)[1:10]
# Display top 10 nodes from the sub-graph ordered by betweenness centrality
sort(betweenness(comp_subgraph, directed = FALSE), decreasing = TRUE)[1:10]
undir_network_graph <- as.undirected(network_graph, mode = "collapse")
louvain_comm <- cluster_louvain(undir_network_graph)
sizes(louvain_comm)
plot(louvain_comm,
undir_network_graph,
vertex.label = V(undir_network_graph)$screen_name,
vertex.size = 4,
vertex.label.cex = 0.7)
eb_comm <- cluster_edge_betweenness(undir_network_graph)
sizes(eb_comm)
# Saving Data for Community Analysis
save.image(file = "social_network_analysis_taylor.RData")
rd_data <- readRDS("~/Griffith/Big Data Analytics/Assignment/Test01_albums/olivia_centrality/rd_data.rds")
network_graph <- readRDS("reddit_actor_olivia.rds")
reddit_actor_olivia <- readRDS("~/Griffith/Big Data Analytics/Assignment/Test01_albums/olivia_centrality/reddit_actor_olivia.rds")
network_graph <- readRDS("reddit_actor_olivia.rds")
rd_actor_network <- rd_data |>
Create("actor") |>
AddText(rd_data,
verbose = TRUE)
# Create graph from the network
rd_actor_graph <- rd_actor_network |> Graph()
rd_actor_graph
V(rd_actor_graph)$name <- V(rd_actor_graph)$user
rd_actor_graph
reddit_actor_olivia <- readRDS("~/Griffith/Big Data Analytics/Assignment/Test01_albums/olivia_centrality/reddit_actor_olivia.rds")
network_graph <- readRDS("reddit_actor_olivia.rds")
# Create graph from the network
rd_actor_graph <- rd_actor_network |> Graph()
rd_actor_graph
V(rd_actor_graph)$name <- V(rd_actor_graph)$user
rd_actor_graph
# Save and write graph to file
saveRDS(rd_actor_graph, file = "reddit_actor_olivia.rds")
write_graph(rd_actor_graph, file = "reddit_actor_olivia.graphml", format = "graphml")
# Load your chosen network graph
network_graph <- readRDS("reddit_actor_olivia.rds")
setwd('~/Griffith/Big Data Analytics/Assignment/Test01_albums/olivia_centrality')
network_graph <- readRDS("reddit_actor_olivia.rds")
length(V(network_graph))
V(network_graph)$name[1:20]
comps <- components(network_graph, mode = c("weak"))
comps$no
comps$csize
head(comps$membership, n = 30)
undir_network_graph <- as.undirected(network_graph, mode = "collapse")
louvain_comm <- cluster_louvain(undir_network_graph)
sizes(louvain_comm)
eb_comm <- cluster_edge_betweenness(undir_network_graph)
sizes(eb_comm)
setwd('~/Griffith/Big Data Analytics/Assignment/Test01_albums/ariana_centrality')
rd_data <- readRDS("~/Griffith/Big Data Analytics/Assignment/Test01_albums/ariana_centrality/rd_data.rds")
network_graph <- readRDS("reddit_actor_ariana.rds")
undir_network_graph <- as.undirected(network_graph, mode = "collapse")
# Run Louvain algorithm
louvain_comm <- cluster_louvain(undir_network_graph)
# See sizes of communities
sizes(louvain_comm)
eb_comm <- cluster_edge_betweenness(undir_network_graph)
sizes(eb_comm)
setwd('~/Griffith/Big Data Analytics/Assignment/Test01_albums')
load("~/Griffith/Big Data Analytics/Assignment/Test01_albums/social_network_analysis_taylor.RData")
library(syuzhet)
# Assign sentiment scores to comments
sentiment_scores <- get_sentiment(clean_text, method = "afinn") |> sign()
sentiment_df <- data.frame(text = clean_text, sentiment = sentiment_scores)
View(sentiment_df)
# Convert sentiment scores to labels: positive, neutral, negative
sentiment_df$sentiment <- factor(sentiment_df$sentiment, levels = c(1, 0, -1),
labels = c("Positive", "Neutral", "Negative"))
View(sentiment_df)
# Plot sentiment classification
ggplot(sentiment_df, aes(x = sentiment)) +
geom_bar(aes(fill = sentiment)) +
scale_fill_brewer(palette = "RdGy") +
labs(fill = "Sentiment") +
labs(x = "Sentiment Categories", y = "Number of Comments") +
ggtitle("Sentiment Analysis of Comments")
# Assign emotion scores to comments
emo_scores <- get_nrc_sentiment(clean_text)[ , 1:8]
emo_scores_df <- data.frame(clean_text, emo_scores)
View(emo_scores_df)
# Calculate proportion of emotions across all comments
emo_sums <- emo_scores_df[,2:9] |>
sign() |>
colSums() |>
sort(decreasing = TRUE) |>
data.frame() / nrow(emo_scores_df)
names(emo_sums)[1] <- "Proportion"
View(emo_sums)
# Plot emotion classification
ggplot(emo_sums, aes(x = reorder(rownames(emo_sums), Proportion),
y = Proportion,
fill = rownames(emo_sums))) +
geom_col() +
coord_flip()+
guides(fill = "none") +
scale_fill_brewer(palette = "Dark2") +
labs(x = "Emotion Categories", y = "Proportion of Comments") +
ggtitle("Emotion Analysis of Comments")
View(emo_scores)
top50_features <- get_playlist_audio_features("spotify", "37i9dQZF1DXcBWIGoYBM5M")
View(top50_features)
data.frame(colnames(top50_features))
top50_features_subset <- top50_features[ , 6:17]
View(top50_features_subset)
top50_features_subset <- top50_features_subset |> rename(track_id = track.id)
# Add the 'isTaylor' column (class variable) to each data frame
# to indicate which songs are by Taylor and which are not
top50_features_subset["isTaylor"] <- 0
taylor_features_subset["isTaylor"] <- 1
# Remove any songs by Taylor that appear in the top 50
# and combine the two data frames into one dataset
top50_features_notaylor <- anti_join(top50_features_subset,
taylor_features_subset,
by = "track_id")
comb_data <- rbind(top50_features_notaylor, taylor_features_subset)
# Format the dataset so that we can give it as input to a model:
# change the 'isTaylor' column into a factor
# and remove the 'track_id' column
comb_data$isTaylor <- factor(comb_data$isTaylor)
comb_data <- select(comb_data, -track_id)
comb_data <- comb_data[sample(1:nrow(comb_data)), ]
split_point <- as.integer(nrow(comb_data)*0.8)
training_set <- comb_data[1:split_point, ]
testing_set <- comb_data[(split_point + 1):nrow(comb_data), ]
dt_model <- train(isTaylor~ ., data = training_set, method = "C5.0")
# Load Package for Decision Tree
library(C50)
library(caret)
library(e1071)
dt_model <- train(isTaylor~ ., data = training_set, method = "C5.0")
prediction_row <- 1 # MUST be smaller than or equal to testing set size
predicted_label <- predict(dt_model, testing_set[prediction_row, ]) # predict the label for this row
predicted_label <- as.numeric(levels(predicted_label))[predicted_label] # transform factor into numeric valu
if (predicted_label == testing_set[prediction_row, 12]){
print(paste0("Prediction is: ", predicted_label, ". Correct!"))
} else {
paste0("Prediction is: ", predicted_label, ". Wrong.")
}
View(comb_data)
if (predicted_label == testing_set[prediction_row, 1]){
print(paste0("Prediction is: ", predicted_label, ". Correct!"))
} else {
paste0("Prediction is: ", predicted_label, ". Wrong.")
}
if (predicted_label == testing_set[prediction_row, 2]){
print(paste0("Prediction is: ", predicted_label, ". Correct!"))
} else {
paste0("Prediction is: ", predicted_label, ". Wrong.")
}
prediction_row <- 1 # MUST be smaller than or equal to testing set size
predicted_label <- predict(dt_model, testing_set[prediction_row, ]) # predict the label for this row
predicted_label <- as.numeric(levels(predicted_label))[predicted_label] # transform factor into numeric value
if (predicted_label == testing_set[prediction_row, 2]){
print(paste0("Prediction is: ", predicted_label, ". Correct!"))
} else {
paste0("Prediction is: ", predicted_label, ". Wrong.")
}
if (predicted_label == testing_set[prediction_row, 3]){
print(paste0("Prediction is: ", predicted_label, ". Correct!"))
} else {
paste0("Prediction is: ", predicted_label, ". Wrong.")
}
if (predicted_label == testing_set[prediction_row, 10]){
print(paste0("Prediction is: ", predicted_label, ". Correct!"))
} else {
paste0("Prediction is: ", predicted_label, ". Wrong.")
}
if (predicted_label == testing_set[prediction_row, 12]){
print(paste0("Prediction is: ", predicted_label, ". Correct!"))
} else {
paste0("Prediction is: ", predicted_label, ". Wrong.")
}
sionMatrix(dt_model, reference = testing_set$isTaylor)
dt_model <- train(isTaylor~ ., data = training_set, method = "C5.0")
prediction_row <- 1 # MUST be smaller than or equal to testing set size
predicted_label <- predict(dt_model, testing_set[prediction_row, ]) # predict the label for this row
predicted_label <- as.numeric(levels(predicted_label))[predicted_label] # transform factor into numeric value
if (predicted_label == testing_set[prediction_row, 12]){
print(paste0("Prediction is: ", predicted_label, ". Correct!"))
} else {
paste0("Prediction is: ", predicted_label, ". Wrong.")
}
confusionMatrix(dt_model, reference = testing_set$isTaylor)
# Remember to save your data
save.image(file = "decision_tree_taylor.RData")
prediction_row <- 12 # MUST be smaller than or equal to testing set size
predicted_label <- predict(dt_model, testing_set[prediction_row, ]) # predict the label for this row
predicted_label <- as.numeric(levels(predicted_label))[predicted_label] # transform factor into numeric value
if (predicted_label == testing_set[prediction_row, 12]){
print(paste0("Prediction is: ", predicted_label, ". Correct!"))
} else {
paste0("Prediction is: ", predicted_label, ". Wrong.")
}
prediction_row <- 16 # MUST be smaller than or equal to testing set size
predicted_label <- predict(dt_model, testing_set[prediction_row, ]) # predict the label for this row
predicted_label <- as.numeric(levels(predicted_label))[predicted_label] # transform factor into numeric value
if (predicted_label == testing_set[prediction_row, 12]){
print(paste0("Prediction is: ", predicted_label, ". Correct!"))
} else {
paste0("Prediction is: ", predicted_label, ". Wrong.")
}
non_zero_entries = unique(doc_term_matrix$i)
dtm = doc_term_matrix[non_zero_entries,]
save(dtm, file = "doc_term_matrix.RData")
rm(list = ls(all.names = TRUE))
gc()
load("doc_term_matrix.RData")
lda_model <- LDA(dtm, k = 9)
load("doc_term_matrix.RData")
load("~/Griffith/Big Data Analytics/Assignment/Test01_albums/social_network_analysis_taylor.RData")
# Assign sentiment scores to comments
sentiment_scores <- get_sentiment(clean_text, method = "afinn") |> sign()
sentiment_df <- data.frame(text = clean_text, sentiment = sentiment_scores)
View(sentiment_df)
# Convert sentiment scores to labels: positive, neutral, negative
sentiment_df$sentiment <- factor(sentiment_df$sentiment, levels = c(1, 0, -1),
labels = c("Positive", "Neutral", "Negative"))
View(sentiment_df)
# Plot sentiment classification
ggplot(sentiment_df, aes(x = sentiment)) +
geom_bar(aes(fill = sentiment)) +
scale_fill_brewer(palette = "RdGy") +
labs(fill = "Sentiment") +
labs(x = "Sentiment Categories", y = "Number of Comments") +
ggtitle("Sentiment Analysis of Comments")
# Assign emotion scores to comments
emo_scores <- get_nrc_sentiment(clean_text)[ , 1:8]
emo_scores_df <- data.frame(clean_text, emo_scores)
View(emo_scores_df)
# Calculate proportion of emotions across all comments
emo_sums <- emo_scores_df[,2:9] |>
sign() |>
colSums() |>
sort(decreasing = TRUE) |>
data.frame() / nrow(emo_scores_df)
names(emo_sums)[1] <- "Proportion"
View(emo_sums)
# Plot emotion classification
ggplot(emo_sums, aes(x = reorder(rownames(emo_sums), Proportion),
y = Proportion,
fill = rownames(emo_sums))) +
geom_col() +
coord_flip()+
guides(fill = "none") +
scale_fill_brewer(palette = "Dark2") +
labs(x = "Emotion Categories", y = "Proportion of Comments") +
ggtitle("Emotion Analysis of Comments")
# ---- 11 Decision Tree ----
# Get top 50 songs and their audio features
top50_features <- get_playlist_audio_features("spotify", "37i9dQZF1DXcBWIGoYBM5M")
View(top50_features)
data.frame(colnames(top50_features))
top50_features_subset <- top50_features[ , 6:17]
View(top50_features_subset)
top50_features_subset <- top50_features_subset |> rename(track_id = track.id)
# Add the 'isTaylor' column (class variable) to each data frame
# to indicate which songs are by Taylor and which are not
top50_features_subset["isTaylor"] <- 0
taylor_features_subset["isTaylor"] <- 1
# Remove any songs by Taylor that appear in the top 50
# and combine the two data frames into one dataset
top50_features_notaylor <- anti_join(top50_features_subset,
taylor_features_subset,
by = "track_id")
comb_data <- rbind(top50_features_notaylor, taylor_features_subset)
# Format the dataset so that we can give it as input to a model:
# change the 'isTaylor' column into a factor
# and remove the 'track_id' column
comb_data$isTaylor <- factor(comb_data$isTaylor)
comb_data <- select(comb_data, -track_id)
# Randomise the dataset (shuffle the rows)
comb_data <- comb_data[sample(1:nrow(comb_data)), ]
# Split the dataset into training and testing sets (80% training, 20% testing)
split_point <- as.integer(nrow(comb_data)*0.8)
training_set <- comb_data[1:split_point, ]
testing_set <- comb_data[(split_point + 1):nrow(comb_data), ]
# Train the decision tree model
dt_model <- train(isTaylor~ ., data = training_set, method = "C5.0")
# Sample a single prediction (can repeat)
prediction_row <- 1 # MUST be smaller than or equal to testing set size
predicted_label <- predict(dt_model, testing_set[prediction_row, ]) # predict the label for this row
predicted_label <- as.numeric(levels(predicted_label))[predicted_label] # transform factor into numeric value
if (predicted_label == testing_set[prediction_row, 12]){
print(paste0("Prediction is: ", predicted_label, ". Correct!"))
} else {
paste0("Prediction is: ", predicted_label, ". Wrong.")
}
# Analyse the model accuracy with a confusion matrix
confusionMatrix(dt_model, reference = testing_set$isTaylor)
# Remember to save your data
save.image(file = "decision_tree_taylor.RData")
non_zero_entries = unique(doc_term_matrix$i)
dtm = doc_term_matrix[non_zero_entries,]
save(dtm, file = "doc_term_matrix.RData")
rm(list = ls(all.names = TRUE))
gc()
load("doc_term_matrix.RData")
lda_model <- LDA(dtm, k = 9)
library(topicmodels)
library(reshape2)
lda_model <- LDA(dtm, k = 9)
found_topics <- tidy(lda_model, matrix = "beta")
library(vosonSML)
library(dplyr)
library(tidyr)
library(tidytext)
library(textclean)
library(tm)
library(ggplot2)
library(igraph)
# Load Packages for Spotify
library(spotifyr)
library(ggridges)
# Load Package for Sentiment Analysis
library(syuzhet)
# Load Package for Decision Tree
library(C50)
library(caret)
library(e1071)
# Load Package for Topic Modelling
library(topicmodels)
library(reshape2)
found_topics <- tidy(lda_model, matrix = "beta")
found_topics <- tidy(lda_model, matrix = "beta")
lda_model <- LDA(dtm, k = 9)
found_topics <- tidy(lda_model, matrix = "beta")
library(tidytext)
found_topics <- tidy(lda_model, matrix = "beta")
lda_model <- LDA(dtm, k = 6)
found_topics <- tidy(lda_model, matrix = "beta")
load("~/Griffith/Big Data Analytics/Assignment/Test01_albums/decision_tree_taylor.RData")
View(doc_term_matrix)
rd_data <- readRDS("rd_data.rds")
rd_data <- rd_data[complete.cases(rd_data), ]
clean_text <- rd_data$comment |> # change 'comment' to 'Comment' for YouTube
replace_url() |>
replace_html() |>
replace_non_ascii() |>
replace_word_elongation() |>
replace_internet_slang() |>
replace_contraction() |>
removeNumbers() |>
removePunctuation()
text_corpus <- VCorpus(VectorSource(clean_text))
text_corpus[[1]]$content
text_corpus <- text_corpus |>
tm_map(content_transformer(tolower)) |>
tm_map(removeWords, stopwords(kind = "SMART")) |>
# tm_map(stemDocument) |> # optional
tm_map(stripWhitespace)
text_corpus[[1]]$content
text_corpus[[5]]$content
doc_term_matrix <- DocumentTermMatrix(text_corpus)
non_zero_entries = unique(doc_term_matrix$i)
dtm = doc_term_matrix[non_zero_entries,]
save(dtm, file = "doc_term_matrix.RData")
rm(list = ls(all.names = TRUE))
gc()
load("doc_term_matrix.RData")
found_topics <- tidy(lda_model, matrix = "beta")
lda_model <- LDA(dtm, k = 6)
found_topics <- tidy(lda_model, matrix = "beta")
found_topics <- tidy(lda_model, matrix = "beta")
library(tidyr)
library(tidytext)
library(textclean)
library(tm)
library(topicmodels)
library(reshape2)
library(dplyr)
library(ggplot2)
found_topics <- tidy(lda_model, matrix = "beta")
# Install the tidytext package if you haven't already
install.packages("tidytext")
# Load the tidytext package
library(tidytext)
lda_model <- LDA(dtm, k = 6)
found_topics <- tidy(lda_model, matrix = "beta")
# Load the tidytext package
library(tidytext)
found_topics <- tidy(lda_model, matrix = "beta")
View(found_topics)
top_terms <- found_topics |>
group_by(topic) |>
slice_max(beta, n = 10) |>
ungroup() |>
arrange(topic, -beta)
library(tidyr)
library(textclean)
library(tm)
library(topicmodels)
library(reshape2)
library(dplyr)
library(ggplot2)
top_terms <- found_topics |>
group_by(topic) |>
slice_max(beta, n = 10) |>
ungroup() |>
arrange(topic, -beta)
top_terms |>
mutate(term = reorder_within(term, beta, topic)) |>
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
# Remember to save your data
save.image(file = "taylor_topic_modelling.RData")
